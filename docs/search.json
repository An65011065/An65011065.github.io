[
  {
    "objectID": "posts/Auditing Allocative Bias/allocative_bias.html",
    "href": "posts/Auditing Allocative Bias/allocative_bias.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "Basic Descriptives\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\ndf.shape\n\n(44742, 7)\n\n\nThe shape that the training dataset consists of 214,480 observations, with each instance having values for 7 different features.\nLet’ see how many of these observations have an income of <= to an income of $50,000. As shown in the output below, 79.39% of people in Colorado have an income of less that or equal to 50,000 dollars.\n\nincome_gt_50k = df[df[\"label\"] == 1].shape[0]\nincome_lt_50k = df[df[\"label\"] == 0].shape[0]\n\ntotal_count = df.shape[0]\nincome_gt_50k_percent = income_gt_50k / total_count * 100\nincome_lt_50k_percent = income_lt_50k / total_count * 100\n\nresults = pd.DataFrame({\n    \"Income\": [\"<= $50,000\", \"> $50,000\"],\n    \"Count\": [income_lt_50k, income_gt_50k],\n    \"Percentage\": [income_lt_50k_percent, income_gt_50k_percent]\n})\n\nprint(results)\n\n       Income  Count  Percentage\n0  <= $50,000  32972   73.693621\n1   > $50,000  11770   26.306379\n\n\nIf we were to delve deeper, we can seethat out of the people who have an income higher than $50,000 dollars, 26.87% are male while 14.56% are females.\n\nincome_gt_50k = df[df[\"label\"] == 1]\nmale_income_gt_50k = income_gt_50k[income_gt_50k[\"group\"] == 1]\nfemale_income_gt_50k = income_gt_50k[income_gt_50k[\"group\"] == 2]\n\nmale_percent = len(male_income_gt_50k) / len(df[df[\"group\"] == 1]) * 100\nfemale_percent = len(female_income_gt_50k) / len(df[df[\"group\"] == 2]) * 100\n\nresults = pd.DataFrame({\n    \"Group\": [\"Male\", \"Female\"],\n    \"Percentage\": [male_percent, female_percent]\n})\n\nprint(results.to_string(index=False))\n\n\n Group  Percentage\n  Male   32.725562\nFemale   19.915254\n\n\nThe results produced from the code below shows the percentage of males that have an income of greater than $50,000 and percentage of female with an income greater than $50,000.\n\nincome_gt_50k = df[df[\"label\"] == 1]\nincome_count = income_gt_50k.groupby(\"group\").size()\ntotal_count = income_gt_50k.shape[0]\nincome_percent = income_count / total_count * 100\n\nresults = pd.DataFrame({\n    \"Group\": [\"Male\", \"Female\"],\n    \"Count\": list(income_count),\n    \"Percentage\": list(income_percent)\n})\n\nprint(results.to_string(index=False))\n\n Group  Count  Percentage\n  Male   7305   62.064571\nFemale   4465   37.935429\n\n\nWe can observe that 64% of the total male have an income greater than $50k while 36% of the total female population has an income greater than $50k. We can see the summarized findings in the chart belowe\n\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\naxs[0].bar([\"Male\", \"Female\"], [male_percent, female_percent], color=[\"#5975a4\", \"#cc8a63\"])\naxs[0].set_ylim(0, 100)\naxs[0].set_ylabel(\"Percentage of Individuals with Income > $50,000\")\naxs[0].set_title(\"Income > $50,000 by Gender\")\n\naxs[1].bar(\"Income > $50K\", income_percent[1], width=0.1, color=\"#5975a4\")\naxs[1].bar(\"Income > $50K\", income_percent[2], bottom=income_percent[1], width=0.1, color=\"#cc8a63\")\naxs[1].set_ylim(0, 100)\naxs[1].set_ylabel(\"Percentage of Individuals\")\naxs[1].set_title(\"Income > $50K by Gender\")\naxs[1].legend([\"Male\", \"Female\"])\nplt.show()\n\n\n\n\nNow, let’s observe the relation of race and sex with income:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Computing proportion of positive labels by race and sex\nrace_proportion = df.groupby([\"RAC1P\", \"group\"], as_index=False)[\"label\"].mean()\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 6))\nax = sns.barplot(data=race_proportion, x=\"RAC1P\", y=\"label\", hue=\"group\")\n\nplt.title(\"Distribution of Race and Sex with Income Over $50K\")\nplt.xlabel(\"Race Group\")\nplt.ylabel(\"Proportion of Individuals with Income Over $50K\")\nhandles, labels = ax.get_legend_handles_labels()\nlabels[0] = \"Male\"\nlabels[1] = \"Female\"\nax.legend(handles, labels, title=\"Group\")\nplt.show()\n\n\n\n\nTHe plot shows that for most race groups, there are more male populations with an income greater than 50k. The only exception to this is race group 4 which is “Alaska Native alone”. Further more we can also observe that Asian alone is the race with the highest proportion of individuals with an income greater than 50k collectively between both the genders.\nLet’s also obeserve the relationship between nativity and Income.\n\n\n# Computing proportion of positive labels by race and sex\ncit_proportion = df.groupby([\"CIT\", \"group\"], as_index=False)[\"label\"].mean()\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Computing proportion of positive labels by citizenship\ncit_proportion = df.groupby([\"NATIVITY\"], as_index=False)[\"label\"].mean()\n\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 6))\nsns.barplot(data=cit_proportion, x=\"NATIVITY\", y=\"label\")\n\nplt.title(\"Distribution of Nativity with Income Over $50K\")\nplt.xlabel(\"Nativity\")\nplt.ylabel(\"Proportion of Individuals with Income Over $50K\")\nplt.gca().set_xticklabels([\"Native\", \"Foreign Born\"])\nplt.show()\n\n\n\n\nAs shown by the bar chart, the poportion of individuals with an income over 50k for Native Born are higher than Foreign Born in Colorado.\n\n\nImplementing a Model\nFor our training data set, we will be using a decision tree model. It utilizes the Scikit-learn library to import the necessary modules, including SVC, LogisticRegression, make_pipeline, StandardScaler, confusion_matrix, cross_val_score, and DecisionTreeClassifier. The code also shows the process tuning a model and cross-validation for a decision tree classifier. It iterates over a range of depth values from 2 to 19. Inside the loop, a pipeline is created using make_pipeline, which includes StandardScaler for data preprocessing and DecisionTreeClassifier with the depth value. Cross-validation scores are calculated using cross_val_score with cross-validation of 10 folds. Finally, best depth is the depth value equating to the highest score.\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nscores = []\ndepths = range(2, 10)\n\nfor depth in depths:\n    pipeline = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=depth))\n    score = cross_val_score(pipeline, X_train, y_train, cv=10)\n    scores.append(score.mean())\n\nbest_depth = depths[np.argmax(scores)]\n\nplt.plot(depths, scores, '-o')\nplt.xlabel('Depth')\nplt.ylabel('Cross-validation score')\nplt.title('Accuracy by depth')\nplt.show()\n\nprint(f\"The best depth is {best_depth} with a CV score of {max(scores)}\")\n\n\n\n\nThe best depth is 9 with a CV score of 0.7433510512632892\n\n\n\nmodel = make_pipeline(StandardScaler(),DecisionTreeClassifier(max_depth=9))\nmodel.fit(X_train,y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=9))])\n\n\nAfter building the model, we can check the accuracy scores:\n\ny_hat=model.predict(X_test)\n\noverall_accuracy = model.score(X_test, y_test)\n\nprint(f\"The overall accuracy of our model is: {overall_accuracy}\")\n\nThe overall accuracy of our model is: 0.7411943500804578\n\n\n\ntn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n\npost_predictive = tp / (tp+fp)\nprint(f\"PPV: {post_predictive}\" )\n\n# calculating fnr and fpr\nfnr = fn / (fn + tp)\nfpr = fp / (fp + tn)\n\nprint(f\"False Negative Rate (FNR): {fnr}\")\nprint(f\"False Positive Rate (FPR): {fpr}\")\n\nPPV: 0.5055118110236221\nFalse Negative Rate (FNR): 0.6697530864197531\nFalse Positive Rate (FPR): 0.11390568319226119\n\n\nThe PPV value indicates the reliability of positive predictions made by our model.This value means that when our model predicts an instance to belong to the positive class, it is correct about 50% times. This suggests that our model has high rate of false positives compared to true positives which means the model is struggling to accurately predict the positive class.\nThe false negative rate of about 67% indicates that our model failed to recognize 60% of actual positive instances, and thus categorized them as negative. Also, 11.3% of actual negative instances were incorrectly classified as positive by our model as indicated by the falce positicve score.\n\n# Calculating accuracy for male\nmale_indices = group_test == 1\nmale_indices = np.squeeze(male_indices)\n\nmale_y_test = y_test[male_indices]\nmale_y_hat = y_hat[male_indices]\n\nmale_accuracy = np.mean(male_y_hat == male_y_test)\n\nmale_tn, male_fp, male_fn, male_tp = confusion_matrix(male_y_test, male_y_hat).ravel()\nmale_fnr = male_fn / (male_fn + male_tp)\nmale_fpr = male_fp / (male_fp + male_tn)\n\nprint(f\"The accuracy score for males is {male_accuracy}\")\nprint(f\"The False Negative Rate (FNR) for males is{male_fnr}\")\nprint(f\"The False Positive Rate (FPR) for males is {male_fpr}\\n\")\n\n# Calculating accuracy female \nfemale_indices = group_test == 2\nfemale_indices = np.squeeze(female_indices)\n\nfemale_y_test = y_test[female_indices]\nfemale_y_hat = y_hat[female_indices]\n\nfemale_accuracy = np.mean(female_y_hat == female_y_test)\n\nfemale_tn, female_fp, female_fn, female_tp = confusion_matrix(female_y_test, female_y_hat).ravel()\nfemale_fnr = female_fn / (female_fn + female_tp)\nfemale_fpr = female_fp / (female_fp + female_tn)\n\nprint(f\"The accuracy for females is {female_accuracy}\")\nprint(f\"The False Negative Rate (FNR) for females is {female_fnr}\")\nprint(f\"The False Positive Rate (FPR) for females is {female_fpr}\\n\")\n\nThe accuracy score for males is 0.7074805288896939\nThe False Negative Rate (FNR) for males is0.6748099891422367\nThe False Positive Rate (FPR) for males is 0.1011144332699103\n\nThe accuracy for females is 0.7740511915269197\nThe False Negative Rate (FNR) for females is 0.6610800744878957\nThe False Positive Rate (FPR) for females is 0.12415595730777608\n\n\n\nAlthough the model performs relatively well for both subgroups, there is a slight difference in accuracy, with females having a slightly higher accuracy compared to males. We will thus examine if our model is accurately calibrated.\n\n\nmale_indices = np.where(group_test == 1)[0]\nmale_y_hat = y_hat[male_indices]\nmale_y_test = y_test[male_indices]\n\n\nfemale_indices = np.where(group_test == 2)[0]\nfemale_y_hat = y_hat[female_indices]\nfemale_y_test = y_test[female_indices]\n\nmatrix_male = confusion_matrix(male_y_test, male_y_hat)\ncalibration_male = matrix_male[0, 0] / matrix_male[0].sum()\nmatrix_female = confusion_matrix(female_y_test, female_y_hat)\ncalibration_female = matrix_female[0, 0] / matrix_female[0].sum()\n\nprint(f\"correctly predicted positive instances for men:  {calibration_male}\")\nprint(f\"correctly predicted positive instances for female:  {calibration_female}\")\n\ncorrectly predicted positive instances for men:  0.8988855667300897\ncorrectly predicted positive instances for female:  0.8758440426922239\n\n\nThe process of Calibration involves checking if the predicted probabilities align with the predicted outcomes. For males, the proportion of correctly predicted positive instances (income greater than $50k) was approximately 0.879. Similarly, for women, the proportion of correctly predicted positive instances was approximately 0.843. Thus, the model appears to be relatively well calibrated for both males and females. In terms of the error rate balance, the fpr for men and fpr for women only has a difference of about 0.03 and about the same for fnr. Hence, the model does not show any significant imbalances in the error rates.\nHuman Resources Departments in companies in Colorado could make a good use of our model to support salary adjustments across different genders. If the model was to show significant biases, it could lead to unfair treatment or discrimination against certain demographic groups. However in case of male and female in our model, there wasn’t a significant bias, hence the impact of deploying your model for large-scale prediction might not cause problems. However, the accuracy scores for male indicates than the model with get it’s predictions wrong for about 29 out of every 100 males and 24 out of every 100 females. These scores will have to be improved, if the model were to be used in any significant industry or decision making process.\nIt is important to consider bias across other features as well and also to consider bias amongst other genders that are not male and female. Hence, I would believe, a lot more experiments are needed to be carried out before implementing this model in any large-scale prediction. Based on my bias audit, the model is not showing any significant problematic bias but as I mentioned earlie,r considering other features and expanding the dataset is essential in using the model for any significant predictions."
  },
  {
    "objectID": "posts/Penguins/index.html",
    "href": "posts/Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Data Cleaning and Preparation\nBelow, the Palmer’s penguin dataset is being prepared to serve as the training set for the machine learning models. The code drops irrelevant columns in the dataset, removes rows with missing values, converts target variable, species, into numbers and scales the quantitative features.\n\nimport itertools\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    # Drop irrelevant columns\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis=1)\n    # Remove rows with missing values and unknown sex\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    # Encode the target variable\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    # One-hot encode categorical features\n    df = pd.get_dummies(df)\n    # Scale the quantitative features\n    scaler = StandardScaler()\n    quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n    df[quant_cols] = scaler.fit_transform(df[quant_cols])\n    return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nimport warnings\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier\n\nwarnings.filterwarnings(\"ignore\")\n\nall_qual_cols = ['Island', 'Clutch Completion', 'Sex']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nbest_score_LR = 0\nbest_score_KNN=0\nbest_score_RF = 0\nbest_score_DT = 0\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]\n    for pair in combinations(all_quant_cols, 2):\n        cols = list(pair) + qual_cols\n        \n        # Training on Logistic Regression\n        LR = LogisticRegression()\n        LR.fit(X_train[cols], y_train)\n        lr_score = LR.score(X_train[cols], y_train)\n\n        # Updating the best score and columns for Logistic Regression\n        if lr_score > best_score_LR:\n            best_cols_LR = cols\n            best_score_LR = lr_score\n            best_LR = LR\n\n        # Training on K-nearest Neighbour Classifier\n        KNN = KNeighborsClassifier()\n        KNN.fit(X_train[cols], y_train)\n        knn_score = KNN.score(X_train[cols], y_train)\n\n        # Updating the best score and columns for k-Nearest Neighbors Classifier\n        if knn_score > best_score_KNN:\n            best_cols_KNN = cols\n            best_score_KNN = knn_score\n            best_KNN = KNN\n        \n        # Training on Random Forest Classifier\n        RF = RandomForestClassifier(max_depth=3, random_state=0)\n        RF.fit(X_train[cols], y_train)\n        rf_score = RF.score(X_train[cols], y_train)\n\n        # Updating the best score and columns for Random Forest Classifier\n        if rf_score > best_score_RF:\n            best_cols_RF = cols\n            best_score_RF = rf_score\n            best_RF = RF\n\n        # Training on Decision Tree Classifier\n        DT = DecisionTreeClassifier(max_depth=3)\n        DT.fit(X_train[cols], y_train)\n        dt_score = DT.score(X_train[cols], y_train)\n\n        # Updating the best score and columns for Decision Tree Classifier\n        if dt_score > best_score_DT:\n            best_cols_DT = cols\n            best_score_DT = dt_score\n            best_DT = DT\n\nprint(\"Best 3 columns for LR: \" + str(best_cols_LR))\nprint(\"Best 3 columns for KNN: \" + str(best_cols_KNN))\nprint(\"Best 3 columns for RF: \" + str(best_cols_RF))\nprint(\"Best 3 columns for DT: \" + str(best_cols_DT))\n\n\nBest 3 columns for LR: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nBest 3 columns for KNN: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\nBest 3 columns for RF: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nBest 3 columns for DT: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nIn this section, I used four Sckit learn’s machine learning models- logistic regression, k-nearest neighbor, random forest, and decision tree classifiers- to find the 3 combination of features that resulted in the highest accuracy scores. The accuracy of each combination of three features was then compared to find the combination that resulted in the highest accuracy score.\n\n# Prepare the test data\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\n\n\ntest = pd.read_csv(test_url)\nX_test, y_test = prepare_data(test)\n\n# Evaluate the performance of each model on the test data\ntest_score_LR =  best_LR.score(X_test[best_cols_LR], y_test)\ntest_score_KNN = best_KNN.score(X_test[best_cols_KNN], y_test)\ntest_score_RF = best_RF.score(X_test[best_cols_RF], y_test)\ntest_score_DT = best_DT.score(X_test[best_cols_DT], y_test)\n\n# Print the test scores\nprint(\"Test score for LR: \" + str(test_score_LR))\nprint(\"Test score for KNN: \" + str(test_score_KNN))\nprint(\"Test score for RF: \" + str(test_score_RF))\nprint(\"Test score for DT: \" + str(test_score_DT))\n\nTest score for LR: 0.9852941176470589\nTest score for KNN: 0.9705882352941176\nTest score for RF: 0.9705882352941176\nTest score for DT: 0.9852941176470589\n\n\nOur trained models were not able to achieve a 100% accuracy score in the test data. However, we identify that Logistic regression and decision tree yeild the best accuracy scored on classifying penguins. MOving forwards we will consider both Logistic regression and decision tree to be the best model for classifying penguins\n\nfrom matplotlib.patches import Patch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n      \n\n\n\nplot_regions(best_LR, X_test[best_cols_LR], y_test)\n\n\n\n\n\nplot_regions(best_DT, X_test[best_cols_DT], y_test)\n\n\n\n\n\nplot_regions(best_KNN, X_test[best_cols_KNN], y_test)\n\n\n\n\n\nplot_regions(best_RF, X_test[best_cols_RF], y_test)"
  },
  {
    "objectID": "posts/Dr. Timnit Gebru/Blog on Dr.Gebru.html",
    "href": "posts/Dr. Timnit Gebru/Blog on Dr.Gebru.html",
    "title": "Dr. Timnit Gebru",
    "section": "",
    "text": "Summary of Dr. Gebru’s Lecture\nIn 2020, as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision, at the conference on Computer Vision and Pattern Recognition, Dr. Timnit Gebru highlighted severe issues related to AI, specifically about Computer Vision. She emphasizes the need for adressing the risks of facial recognition systems not only in terms of how the technogy is developed but also how the technology is being deployed. Through out her lecture, Dr. Gebru uses exmples such as the use of biased facial recognition softwares by Maryland police or Amazon advocating the use of these softwares in manners they were not meant to be used initially.\nDr. Gebru spoke adressed that eliminating bias in facial recognition systems does not mean just diversifying a dataset. She highlighted that ethical and social considerations also need to be taken into account. Gebru pointed to the example of transgender women’s YouTube profiles being used to train facial recognition systems, which raises ethical concerns about the use of personal data.\nDr. Gebru also emphasized that acknowledging the presence of bias does not necessarily eradicate it in the system. In some cases, making a system fair does not mean making it work equally well for everyone, as the fundamental concept of the system could be flawed. She gave an example of how flawed tools like facial recognition systems have been used to hurt innocent people and even used as evidence in court cases.\nPointing to companies like Amazon who have failed to ensure that their facial recognition technology is not biased, Dr.Gebru questions the need to understand who is developing and promoting these technologies. I personally feel like companies should not promote their product, knowing well that their product has defects. Amazon promoting their biased facial recognition software to police bodies is like selling a faulty compass to a lost hiker - it only leads them further astray. Dr. Gebru also highlights that many ethics boards in companies who are working on AI only represent the most dominant and powerful groups, leaving those who are at the negative receiving end of these technologies out of the conversation.\nGebru emphasized that fairness is not all about datasets or maths but also about society and its ethics. It’s important to know how are these technoligies being developed and how are the technologies being deployed. People working in computer vision need to understand how dangerously their work is being used and take responsibility that it is being deployed in an ethical manner.\n\n\nQuestions\n\nEarlier in your career, you wondered how tech giants like apple manage[d] to avoid scrutiny for their failure to address and eradicate unfairness within their organizations.Despite occasional concerted efforts from employees or other stakeholders, such as workers’ unions, it appears that little had improved within the technology industry. What factors, do you think, are allowing tech industries to get away from taking accountability and addressing these issues?\nIf tech industries were to develop AI in the same manner as it is being done today, would you think it would be better to completely stop this technology or do you think the benefits of its existence outweights the concerns surrounding it?\nToday, AI related projects are potentially growing exponentially and given the concerns of its misuse, what are the most urgent steps that companies but also its users need to take to ensure AI is developed, and used in a fair manner.\nThe capabilities of AI that we see today is far different that its capibilities when AI was first introduced. Moreover, AI is a technology that relies on years of study done on it. How far would we have to go to completely eliminate any form of bias in today’s AI.\n\n\n\nReflection on Dr. Gebru’s Talk\nDr. Gebru’s talk was based on the pillars of her organization Distributed AI Research Institute(DAIR), which centers around understanding and mitigating the harms of current AI systems and imagining a different technological future. Her talk featured quotes from popular figures in the AI industry such as Sam Altman and Karen Hao. Dr. Gebru shed some much-needed light on Open AI’s and other AI companies immoral practices such as exploting cheap labour from poor nations and stealing content from artists to train their models without any permission or compensations. Although I had acquired a good understanding of these topics by following renowned experts who work on AI biasness on Twitter, Dr. Gebru introduced me to a range of whole new topics such as Eugenics, Transhumanism, and Effective Altruism that I had previously never looked into.\nDr. Gebru’s perspective on AI and its development being rooted in the 20th century Anglo American eugenic tradition is certainly an interesting viewpoint. Based on my understanding, Dr. Gebru stated that the combination of eugenics and cosmism rooted on sentient AI, would introduce a new phase of the evlution of human species. However, I was left uncertain as to why eugenics would be necessary in this scenario, given powerful technologies should theoretically be able to adapt to all individuals without the need for policies aiming to improve human stock or restrict miscegenation. The conversation on the TESCREAL bundle did leave me feeling, however, it also opened a world of new conversations/topics that I never knew had connections to AI.\nPersonally, I found the conversation about Utopia and who this Utopia is being created for thought-provoking. The prospect of gaining wealth and intelligience through AI is very lucritive for every individual. However, Dr. Gebru raised a great question: what if the benefits of these technologies are eventually be limited to the 1% population of the world, who in my perspective are doing well without the existence of AI. Prior to attending the lecture, I never considered the potential consequences of contributes to training a model but I definitely do now.\nI really found it insightful when Dr. Gebru raised the history of companies evading accountability and the ease with which comapnies devoloing AGI’s will be able to evade accountability for the biasness in their models. I agree with her notion that anthropomorphizing artifacts allows builders to evade responsibility and there should definitely be policies placed well on time. Dr. Gebru’s experiences with sexism and harassment in the AI industry, such as facing resistance when appealing for the renaming of a conference named after adult websites, are troubling and highlight the urgent need for greater inclusion and accountability in the field. In addition, the lack of diversity within the AI industry is a cause for concern. When models are developed exclusively by individuals of a particular gender, race, or other demographic factors, they may not prioritize checking for biases that affect other groups, which could lead to detrimental outcomes. Also, I believe that there has to be limitations on how AI Models obtain their training data and who can build these models. I remember when DALL-E was first introduced, it caused major controversy due to its failure in removing signatures/watermarks from artworks that it was using to generate new artworks. Such instances have the potential to diminish the work of artists who have invested significant resources into creating their own original artworks. Dr. Gebru’s example of the language detection model developed by Ghana ALP, which outperformed Microsoft’s model in terms of BLEU score, highlights the need for increased awareness around smaller companies and organizations working towards advancements in the AI industry. It is of utmost importance to understand that larger tech companies such as Microsoft hold significant power and influence within the industry, which could completely eliminate the existence of smaller entities.\n\n\nConclusion\nDr. Timnit Gebru has been a prominent voice in raising awareness on ethical AI development. The knowledge that she shared in the lecture must have been a cumilation of the years of experience she has had in the industry. While individuals may hold varying perspectives on topics such as altruism and eugenics, it should not be unacceptable to tolerate the development of biased models that are trained using unethical practices. The fact that such models have the potential to overshadow entire industries is a greater cause for concern. I believe that there is a need for organizations like Future of Life to exist and have more governing power on who gets to develop AI models and the core procedures/tests that all organizations must follow to ensure that their model is unbiased and ethical.\nAI has the potential to solve many modern world problems. Dr. Gebru gave examples of work done at NYU AD in detecting images of bombs and in Costa Rica where AI is used to recognize plants and improve agricture. However, a technology that has the potential to be so powerful will also has/will have major shortcomings. Hence, AI technologies must be regulated and it is crucial to encourage independent researchers like Dr. Timnit Gebru, as they play a pivotal role in ensuring that the AI models that the world will rely on in the future are developed in an unbiased and ethical manner."
  },
  {
    "objectID": "posts/Linear Regression/index.html",
    "href": "posts/Linear Regression/index.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "Implementation\nLet’s utilize our the analytic_fit method to first test the analytical method. However, we need to first create a set of training data and validation data.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt \n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    #creating a random X_train matrix with n_train rows - data points - and p_features columns - the features\n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nTo make visualization simpler, we are setting the number of features to 1:\n\nX_train, y_train, X_val, y_val = LR_data(n_train = 100, n_val = 100, p_features = 1, noise = 0.1)\n\n#creating a padded version of X_train and y_train\nX_train_padded = pad(X_train)\nX_val_padded = pad(X_val)\n\nHere, we are visualizing the data:\n\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\nplt.rcParams[\"figure.figsize\"] = (15,5)\naxarr[0].scatter(X_train, y_train, color=\"purple\")\naxarr[1].scatter(X_val, y_val, color=\"purple\")\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\", ylabel = \"y\")\nplt.tight_layout()\n\n\n\n\nWe will use our Linear_Regression method in the Linear_Regression Class, to see how our implementation performs on the training and testing data. Printed below is the weight vector, accuracy score on the training data and the validation data.\n\nfrom Linear_Regression import Linear_Regression\n\nlr = Linear_Regression()\n\n# use the methods of the class\nlr.analytic_fit(X_train, y_train)\n\nprint(f\"Weight vector: {lr.coefficients}\")\nprint(f\"Training score: {lr.accuracy_score(X_train, y_train).round(4)}\")\nprint(f\"Validation score: {lr.accuracy_score(X_val, y_val).round(4)}\")\n\nWeight vector: [1.14598322 0.98235482]\nTraining score: 0.881\nValidation score: 0.8784\n\n\nThe Linear Regression implementation using analytic method seems to do well with both our training and testing data. Let’s see, if that is the case with the gradient descent method:\n\nfrom Linear_Regression import Linear_Regression\n\nlr2 = Linear_Regression()\n\nlr2.gradient_fit(X_train, y_train)\n\nprint(f\"Weight vector: {lr2.coefficients}\")\nprint(f\"Training score: {lr2.accuracy_score(X_train, y_train).round(4)}\")\nprint(f\"Validation score: {lr2.accuracy_score(X_val, y_val).round(4)}\")\n\nWeight vector: [1.14628734 0.98183735]\nTraining score: 0.881\nValidation score: 0.8784\n\n\nBoth the methods result in the same weight vector and a similar accuracy scores on both the training and validation data. This shows that our implementation is correct because we are solving the same optimization problem.\n\n\nGradient Descent: Accuracy Score over Iterations\n\nplt.plot(lr2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nIn our case, we are using the entire dataset in each iteration to calculate the gradient of the cost function, ensuring a consistent improvement in accuracy over iterations (We can see this in the graph above)- as opposed to Stochastic Gradient Descent which utilizes only a single or a few random data points per iteration.\n\n# Experiment: Number of Features and Accuracy Scores\n\n\nimport matplotlib.pyplot as plt\n\nn_train = 100  \n\nlr3 = Linear_Regression()\ntrain_scores = []\nval_scores = []\n\nfor i in range(1, n_train):\n    X_train, y_train, X_val, y_val = LR_data(n_train = 100, n_val = 100, p_features = i, noise = 0.1)\n    lr3.gradient_fit(X_train, y_train)\n    train_scores.append(lr3.accuracy_score(X_train, y_train))\n    val_scores.append(lr3.accuracy_score(X_val, y_val))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, n_train), train_scores, label='Training score')\nplt.plot(range(1, n_train), val_scores, label='Validation score')\nplt.title('Training and Validation score vs. Number of features (Gradient Descent Approach)')\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nIn our original implementation, our model had only one feature. To explore how the number of features influences model performance, we generated datasets with up to 100 features. For each dataset, we increased the number of features by one and calculated the accuracy score on both the training and validation sets.\nThe model’s performance on the training set improved with the addition of each new feature, reaching a near-perfect training score. This is because more features provide the model with more information, allowing it to fit the training databetter. However, in the validation score increased along with the training score, suggesting that the additional features were relating meaningful patterns but after the number of features exceeded 50, the validation score experienced a sharp decline.\nThis is an example of overfitting. Overfitting occurs when a model learns the training data too closely, including its noise and then fails to adapt appropriately to new data. Overfitting can also be observed in the analaytic approach of linear regression\n\nimport matplotlib.pyplot as plt\n\nn_train = 100  \n\nlr3 = Linear_Regression()\ntrain_scores = []\nval_scores = []\n\nfor i in range(1, n_train):\n    X_train, y_train, X_val, y_val = LR_data(n_train = 100, n_val = 100, p_features = i, noise = 0.1)\n    lr3.analytic_fit(X_train, y_train)\n    train_scores.append(lr3.accuracy_score(X_train, y_train))\n    val_scores.append(lr3.accuracy_score(X_val, y_val))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, n_train), train_scores, label='Training score')\nplt.plot(range(1, n_train), val_scores, label='Validation score')\nplt.title('Training and Validation score vs. Number of features ( analytic approach)')\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\nImplementing LASSO\nLASSO is a type of Linear Regression but it implements regularization too. The inclusion of a regularization term in LASSO makes it more robust to overfitting. Let’s see how LASSO deals with overfitting:\n\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.linear_model import Lasso\n\n# Ignore warnings\nwarnings.filterwarnings('ignore')\n\nalpha_values = [0, 0.001, 0.01, 0.1]\n\nfig, axs = plt.subplots(2, 2, figsize=(15, 10))  \naxs = axs.flatten() \n\nfor j, alpha in enumerate(alpha_values):\n    train_scores = []\n    val_scores = []\n\n    for i in range(1, 100):\n        X_train, y_train, X_val, y_val = LR_data(n_train=100, n_val=100, p_features=i, noise=0.1)\n        \n        L = Lasso(alpha=alpha)\n        L.fit(X_train, y_train)\n\n        train_score = L.score(X_train, y_train)\n        val_score = L.score(X_val, y_val)\n        \n        train_scores.append(train_score)\n        val_scores.append(val_score)\n\n    axs[j].plot(range(1, 100), train_scores, label=f'Training score')\n    axs[j].plot(range(1, 100), val_scores, label=f'Validation score')\n    axs[j].set_title(f'Alpha = {alpha}')\n    axs[j].set_xlabel('Number of features')\n    axs[j].set_ylabel('Score')\n    axs[j].legend()\n    axs[j].grid()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nWhen alpha is 0, it indicates no regularization. Hence the outcome should be no different than the standard linear regression model. As the alpha- the variable that controls regularization- increases, the performance on the performance on validation data is more consistent. This indicates that regularization in linear regression models is significant in combating overfitting."
  },
  {
    "objectID": "posts/Perceptron/index.html",
    "href": "posts/Perceptron/index.html",
    "title": "Perceptron Algorithm",
    "section": "",
    "text": "Introduction to Perceptron Algorithm\nThe perceptron algorithm is a powerful machine learning tool used to classify objects as part of a group. It’s application in machine learning algorithm can be extended to real-life scenarios, such as assessing the likelihood of a student’s acceptance into Middlebury College based on their academic performance. In this blog post, we will be exploring how this algorithm works and can be implemented on a linearly seperable data.\n\n\nUnpacking the Perceptron Algorithm\nThe perceptron algorithm separates the data points by a hyperplane, which is a decision boundary that separates the data points into two classes. The weight function w is used to determine the position of the hyperplane.\nWe start with initializing the weights to random values using the np.random.rand() function. We then continue updating the weight vector until all the data points are correctly classified. This is achieved by iteratively selecting a random data point and updating the weight vector until all data points are correctly classified.\nIn order to classify the data points correctly, the perceptron algorithm iteratively updates the weight vector until all data points are classified correctly. To check whether a data point is misclassified, the algorithm starts by choosing a random data point at i- generated by using np.random.randint(m) and uses the comparator:\n                       if (y_tilde[i] * np.dot(X_[i], self.w)) <= 0:\nThe dot product of X_[i] and self_w is computed, resulting in a weighted sum of the features of X_[i]. The sign of this weighted sum determines the predicted label for X_[i]. If a data point is misclassified, the algorithm updates the weight vector by adding or subtracting a scaled version of the misclassified data point using the equation:\n                             self.w += y_tilde[i] * X_[i]\nThis aligns the weight vector with the misclassified data point, pushing the hyperplane towards better classification.\nThe weighted sum is multiplied by the true label (y_tilde), and if the result is 1, the data point is correctly classified. If the result is less than 0, the data point is misclassified, and the weight vector is updated.\nAfter each iteration of updating our weight vector, our algorithm computes the accuracy of the current classification of the hyperplane by using the score function and records the accuracy n the history array.\nThe algorithm continues to update the weight vector until all data points are classified correctly, hence reaching an accuracy of 1, or until the maximum number of iterations is reached.\n\n\nExperiment 1:\nImplementing the Perceptron Algorithm on Linearly Separable Data\nIn the graphs below, the perceptron algorithm is used to find a line that separates a linearly separable dataset with two features. The algorithm iteratively updates the weight vector until all data points are classified correctly. Running the perceptron.fit function on the dataset results in the weight vector converging to a value that defines this separating line. The resulting graphs will visually demonstrate the success of the algorithm in separating the data points.\n\n\nfrom Perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\n#np.random.seed(12345)\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThe decision boundary is the line which separates two classes of data points. In this case, the Perceptron algorithm has successfully found a line that separates the data points into two distinct classes. Hence the perceptron algorithm is succefull on data with two features.\nAlso the accuracy of the algorithm was 1.0 and we can look over the final 10 accuracies until the weight vector converged to a value that defines this separating line.\n\nprint(\"The accuracyscore is \", p.score(X,y))\nprint(\"The final 10 accuracy scores were: \",p.history[-10:]) \n\nThe accuracyscore is  1.0\nThe final 10 accuracy scores were:  [0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 1.0, 1.0]\n\n\nThe graph below plots the accuracy scores from the first iteration until the point of converge:\n\nplt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nExperiment 2:\nImplementing the Perceptron Algorithm on a non-Linearly Separable Data\n\nfrom Perceptron import Perceptron\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_circles\n\nnp.random.seed(12345)\n\nn = 100 \np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) \nxlab = plt.xlabel(\"Feature 1\") \nylab = plt.ylabel(\"Feature 2\")\n\np = Perceptron()\n\np.fit(X, y, max_steps = 1000)\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.title(\"Perceptron classifier's decision boundary on nonlinearly seperable data\")\n\nText(0.5, 1.0, \"Perceptron classifier's decision boundary on nonlinearly seperable data\")\n\n\n\n\n\nBased on the plot itself, it is obvious that the the weight vector found by the Perceptron algorithm did not converge to a straight line that can completely separate the two classes. Let’s check the final accuracy and the final 10 accuracy before the p.fit function reached the maximum number of steps.\n\nprint(\"The accuracyscore is \", p.score(X,y))\nprint(\"The final 10 accuracy scores were: \",p.history[-10:]) \n\nThe accuracyscore is  0.91\nThe final 10 accuracy scores were:  [0.91, 0.91, 0.91, 0.91, 0.91, 0.91, 0.91, 0.91, 0.91, 0.91]\n\n\nBased on the results, it is evident that the p.fitfunction did not terminate due to achieving an accuracy of 1, but rather due to reaching the maximum number of iterations set at 1000. Hence, the perceptron algorithm fails to find a line to separate non-linearly separable data. Furthermore, the graph below- accuracy scores over iterations- shows that the algorithm failed to reach an accuracy score of 1 at any step during the iteration.\n\nplt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\nTime Complexity\nThe runtime complexity of the algorithm is determined by the dot product operation in the predict function. This dot product operation requires O(p) time, where p is the number of features in the input vector. This is because the dot product involves summing the product of each element in the input vector with its corresponding weight in the weight vector, and there are p such products to compute. Thus, as the number of features increases, the runtime of this algorithm increases linearly"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Unsupervised Learning with Linear ALgebra/index.html",
    "href": "posts/Unsupervised Learning with Linear ALgebra/index.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "Converting image to Grayscale\nHere I have chosen a RGB colored image of Mario and converted it to a grayscale image using the PIL package.\n\nfrom PIL import Image\nfrom matplotlib import pyplot as plt \nimport urllib\nimport numpy as np\n\ndef read_image(url):\n    return np.array(Image.open(urllib.request.urlopen(url)))\n\nurl = \"https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/6b51c512-9408-4f6c-9b9d-8bbddbafe45c/dfx6346-aa22997e-3fac-4663-88e8-581060c0dcf0.png/v1/fill/w_988,h_809/gcn_mario_kick__rgb__by_giltchyboi64_dfx6346-pre.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9OTE5IiwicGF0aCI6IlwvZlwvNmI1MWM1MTItOTQwOC00ZjZjLTliOWQtOGJiZGRiYWZlNDVjXC9kZng2MzQ2LWFhMjI5OTdlLTNmYWMtNDY2My04OGU4LTU4MTA2MGMwZGNmMC5wbmciLCJ3aWR0aCI6Ijw9MTEyMyJ9XV0sImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl19.Vb9e3dCybP27DsdJD3yf2MYC_M0MCm1B69gb9Wp0C-0\"\nimg = read_image(url)\n\nfig, axarr = plt.subplots(1, 2, figsize=(7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[..., :3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title=\"original\")\n\naxarr[1].imshow(grey_img, cmap=\"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title=\"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\n\nfrom PIL import Image\nfrom matplotlib import pyplot as plt \nimport urllib\nimport numpy as np\n\nLet’s look at the shapes of the original image and grayscale image:\n\na=np.shape(img)\nb=np.shape (grey_img)\nprint(f\"Shape of original image: {a}\")\nprint(f\"Shape of original image: {b}\")\n\nShape of original image: (809, 988, 4)\nShape of original image: (809, 988)\n\n\nThe grayscale image retains the same width and height but loses the color channels. Now the grayscale image serves as a 2-dimensional matrix, which can be input into the SVD.\n\n\nSVD Implementation\n\ndef svd_reconstruct(image, k):\n    # Performing SVD\n    U, s, V = np.linalg.svd(image, full_matrices=True)\n    D = np.zeros(image.shape)\n    np.fill_diagonal(D, s)\n    \n    # Selecting\n    D_k = D[:k, :k]\n    U_k = U[:, :k]\n    V_k = V[:k, :]\n\n    # Reconstructing our image\n    reconst_img = U_k @ D_k @ V_k\n    \n    return reconst_img\n\n\nk = 50 \nreconst_img = svd_reconstruct(grey_img, k)\n\nfig, axarr = plt.subplots(1, 2, figsize=(12, 6))\n\naxarr[0].imshow(grey_img, cmap='Greys')\naxarr[0].axis('off')\naxarr[0].set_title('Original Image')\n\n#reconstructing image using 50 singular values\naxarr[1].imshow(reconst_img, cmap='Greys')\naxarr[1].axis('off')\naxarr[1].set_title('Reconstructed Image with k = ' + str(k))\n\nplt.show()\n\n\n\n\nAt this k value, we can already see the image coming together which indicates that using only the 50 most important features can reconstruct our image to a decent level.\n\n\nExperimentation\nBy setting k to 50, we are saying that we want to reconstruct our image using only the top 50 most important features. This compresses the image because fewer features are used to construct the image.\nAs k increases, the reconstructed image will become more and more similar to the original because more features are used in the reconstruction. Oppositely, As k decreases, less information is used in the reconstruction, and the image becomes more and more compressed. We can see this in the graph below\n\nimport matplotlib.pyplot as plt\n\ndef storage_percentage(original, k):\n    return (k * (original.shape[0] + original.shape[1]) + k) / (original.shape[0] * original.shape[1]) * 100\n\n# Creating the subplots\nfig, axarr = plt.subplots(2, 4, figsize=(20, 10))\n\n#original iamge\naxarr[0, 0].imshow(grey_img, cmap=\"Greys\")\naxarr[0, 0].axis(\"off\")\naxarr[0, 0].set_title(\"Original Image\\n100% Storage\")\n\n#no. of features\nk_values = [1,5, 25, 50,100, 175, 250]\n\n\nfor i, k in enumerate(k_values, start=1):\n    # Reconstructing the image\n    reconst_img = svd_reconstruct(grey_img, k)\n\n    row = i // 4\n    col = i % 4\n\n    axarr[row, col].imshow(reconst_img, cmap=\"Greys\")\n    axarr[row, col].axis(\"off\")\n    axarr[row, col].set_title(f\"Reconstructed Image\\n k = {k}\\n{storage_percentage(grey_img, k):.2f}% Storage\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nAs mentioned earlier, the selection of k is a balance between reducing storage space (lower k) and preserving image quality (higher k). It is interesting to see how efficiently an image can be compressed/reconstructed without losing much details from the original greyscale image. Moreover, there is a little observational difference when k is set to 175 and 250, however the percentatge of storage is significantly different. At the value of k =100, the image is easily recognizable while only taking 22.39% of the original storage space.\n\n\nOptional Extras:\nThe compression factor is the percentage of singular values retained in the reconstruction. For example, a compression factor of 0.50 would mean retaining 50% of the original singular values. In the code below, we modified our svd_reconstruct method for users to specify a desired compression factor and select the number of components k to use based on this selection. I have demonstrated an example with a compression factor of 0.08\n\ndef no_of_features(compression_factor, img):\n    height, width = img.shape\n    original_storage = height * width\n    target_storage = compression_factor * original_storage\n    k = target_storage / (height + width + 1)\n    return round(k)\n\ncompression_factor = 0.08\nreconst_img = svd_reconstruct(grey_img, no_of_features(compression_factor, grey_img))\n\n\n\n\n\n# Creating subplots\nfig, axarr = plt.subplots(1, 2, figsize=(12, 6))\n\naxarr[0].imshow(grey_img, cmap='Greys')\naxarr[0].axis('off')\naxarr[0].set(title='Original Image')\n\naxarr[1].imshow(reconst_img, cmap='Greys')\naxarr[1].axis('off')\naxarr[1].set(title=f'Reconstructed Image\\n k = {no_of_features(compression_factor, grey_img)}\\n compression_factor= {compression_factor}')\nplt.tight_layout()\nplt.show()\n#\n\n\n\n\n\n\nFinal Remarks\nThe use of SVD and the implementation of the svd_reconstruct function allow efficient image compression and uncover hidden structures in the features of the data. The ability to control the level of reconstruction could be a great tool for various image processing applications."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Understanding different approaches to implement the Linear Regression Model\n\n\n\n\n\n\nMay 22, 2023\n\n\nAnweshan Adhikari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nReconstructing an image using Singular Value Decomposition (SVD)\n\n\n\n\n\n\nMay 22, 2023\n\n\nAnweshan Adhikari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog on Dr.Gebru.\n\n\n\n\n\n\nApr 18, 2023\n\n\nAnweshan Adhikari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nExploring the impact of algorithmic decision making on equality\n\n\n\n\n\n\nApr 11, 2023\n\n\nAnweshan Adhikari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMaximizing Penguin Classification Accuracy with Minimal Features\n\n\n\n\n\n\nApr 9, 2023\n\n\nAnweshan Adhikari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA simple implementation of Perceptron Algorithm.\n\n\n\n\n\n\nMar 10, 2023\n\n\nAnweshan Adhikari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Reach out to me on platforms below!"
  }
]